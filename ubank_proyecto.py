# -*- coding: utf-8 -*-
"""ubank_proyecto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bSS6Jooa8bkeOAhsjPnB2yE0JeTVzrcs

#DESAFIO UBANK
"""

import pandas as pd
import os
import gc
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

ROOT_PATH = "/content/drive/My Drive/Colab Notebooks/datos_ubank/"
DATA_PATH = os.path.join(ROOT_PATH,"catalog.csv")
dataset = pd.read_csv(DATA_PATH)

dataset.info()

dataset.head()

ROOT_PATH_1 = "/content/drive/My Drive/Colab Notebooks/datos_ubank/"
DATA_PATH_1 = os.path.join(ROOT_PATH_1, "test_projects.csv")
dataset_1 = pd.read_csv(DATA_PATH_1)

dataset_1.info()

dataset_1.head()

ROOT_PATH_2 = "/content/drive/My Drive/Colab Notebooks/datos_ubank/"
DATA_PATH_2 = os.path.join(ROOT_PATH_2, "test_rules.csv")
dataset_2 = pd.read_csv(DATA_PATH_2)

dataset_2.info()

dataset_2.head()

ROOT_PATH_3 = "/content/drive/My Drive/Colab Notebooks/datos_ubank/"
DATA_PATH_3 = os.path.join(ROOT_PATH_3, "test_transactions.csv")
dataset_3 = pd.read_csv(DATA_PATH_3)

dataset_3.info()

dataset_3.head()

dataset_3.groupby("user_id")["amount"].mean().plot()

"""#***UNION DE DATASETS***

"""

testproject_catalog=pd.merge(dataset_1,dataset,left_on='project_category_id',right_on='id')
testproject_catalog.head()

testproject_catalog.info()

df=pd.merge(testproject_catalog,dataset_2,on='project_id')
df.head(10)

print('HAY',df['project_id'].isnull().sum(),'VALORES NULOS')

df.info()

result = pd.merge(testproject_catalog,dataset_2,on='project_id', how='outer',indicator=True)
result.info()

result

print('HAY',result['amount'].isnull().sum(),'VALORES NULOS')

df.groupby("user_id")["project_id"].count()

df[df.user_id=='0009062215dd4e809c40b52bdfa390be']

df.groupby("user_id")["rule_id"].count()

"""#ELIMINAR COLUMNAS """

#df = df.drop(['project_category_id','id','rule_id'],axis=1)

df

df['monto_ahorro']=df['amount']*df['frecuency']
df

df_user=df.groupby('project_id')['monto_ahorro'].sum()
df_user.head()

df_unido = pd.merge(df,df_user,on='project_id')
df_unido

"""#ETIQUETAR DATASET"""

import numpy as np
df_unido['ahorro'] = np.where(df_unido['total'] <= df_unido['monto_ahorro_y'] , 1, 0)
df_unido.head(20)

df_unido=df_unido.drop(['name_x','amount','frecuency'],axis=1)

#df_join=pd.get_dummies(df_unido, columns=['rule_type_id'])
#df_join.head()





df_join = df_unido.drop(['categories','monto_ahorro_x'],axis=1)

df_join

df_join.groupby(['rule_type_id']).size().plot.bar(color = 'green')

pd.crosstab(index=df_join['ahorro'] ,columns=df_join['rule_type_id']).apply(lambda r: r/r.sum() *100,axis=1)

pd.crosstab(index=df_join['ahorro'] ,columns=df_join['rule_type_id']).apply(lambda r: r/r.sum() *100,axis=1).plot(kind='bar')

from scipy.stats import chi2_contingency

contingency_tab_product = pd.crosstab(index=df_join['ahorro'] ,columns=df_join['rule_type_id']) 
print("\n Tabla de Contingencia : Chi^2")
table_Chi2_isfraud_product = chi2_contingency(contingency_tab_product, correction=False)
print(table_Chi2_isfraud_product[0])
print(table_Chi2_isfraud_product[1])
print(table_Chi2_isfraud_product[2])
print(table_Chi2_isfraud_product[3])

pd.crosstab(index=df_join['ahorro'] ,columns=df_join['name_y']).apply(lambda r: r/r.sum() *100,axis=1)

pd.crosstab(index=df_join['ahorro'] ,columns=df_join['name_y']).apply(lambda r: r/r.sum() *100,axis=1).plot.bar()

from scipy.stats import chi2_contingency

contingency_tab_product = pd.crosstab(index=df_join['ahorro'] ,columns=df_join['name_y'])
print("\n Tabla de Contingencia : Chi^2")
table_Chi2_isfraud_product = chi2_contingency(contingency_tab_product, correction=False)
print(table_Chi2_isfraud_product[0])
print(table_Chi2_isfraud_product[1])
print(table_Chi2_isfraud_product[2])
print(table_Chi2_isfraud_product[3])

df_join = df_join.drop(['rule_type_id'],axis=1)

"""#ELIMINAR DUPLICADOS"""

df_join=df_join.drop_duplicates()

df_join

from scipy.stats import chi2_contingency

contingency_tab_product_1 = pd.crosstab(index=df_join['ahorro'] ,columns=df_join['type'])
print("\n Tabla de Contingencia : Chi^2")
table_Chi2_isfraud_product_1 = chi2_contingency(contingency_tab_product_1, correction=False)
print(table_Chi2_isfraud_product_1[0])
print(table_Chi2_isfraud_product_1[1])
print(table_Chi2_isfraud_product_1[2])
print(table_Chi2_isfraud_product_1[3])



df

df_join['name_y'].unique()

ahorro_personas = df_join.groupby(['ahorro']).size()
ahorro_personas.plot.bar()

df['user_id'].value_counts()

dataset_3.head()

transactions=dataset_3.groupby("user_id")["amount"].sum()
transactions

#df_final_1=pd.merge(df_join,transactions,on='user_id')

df_final=pd.merge(df_join,transactions,on='user_id')

df_final

ahorro_personas = df_final.groupby(['ahorro']).size()
ahorro_personas.plot.bar()

df_final = df_final.drop(['project_id','goal_date','user_id','rule_id','project_category_id','id'],axis=1)

#df_final_1['amount'].max()
#df_final_1  = df_final.drop(df_final.index[df_final.monto_ahorro_y > 800000.00], axis=0)
#df_final_1.loc[df_final_1.amount == 1531760.87, :]
#df_final_1= df_final.drop(['project_id','goal_date','user_id'],axis=1)

df_final.info()

from matplotlib import pyplot as plt
X=df_final_1['monto_ahorro_y']
Y=df_final_1['amount']
plt.scatter(X,Y)

df_final=pd.get_dummies(df_final)

df_final

from sklearn.preprocessing import MinMaxScaler

"""#NORMALIZAR"""

df_final[['total']]=MinMaxScaler(feature_range=(0,1)).fit_transform(df_final[['total']])

df_final[['monto_ahorro_y']]=MinMaxScaler(feature_range=(0,1)).fit_transform(df_final[['monto_ahorro_y']])

df_final[['amount']]=MinMaxScaler(feature_range=(-1,1)).fit_transform(df_final[['amount']])

df_final

from matplotlib import pyplot as plt
X=df_final['amount']
Y=df_final['monto_ahorro_y']
plt.scatter(X,Y)

df.groupby("user_id")["rule_type_id"].count().plot()

df.groupby("user_id")["project_id"]

from sklearn import datasets, metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report


x=np.array(df_final.drop(['ahorro'],1)) 
y=np.array(df_final['ahorro'])
 
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
 
kf = KFold(n_splits=5)
 
clf = LogisticRegression()
 
clf.fit(X_train, y_train)
 
score = clf.score(X_train,y_train)
 
print("Metrica del modelo", score)
 
scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring="accuracy")
 
print("Metricas cross_validation", scores)
 
print("Media de cross_validation", scores.mean())
 
preds = clf.predict(X_test)
 
score_pred = metrics.accuracy_score(y_test, preds)
 
print("Metrica en Test", score_pred)

print(classification_report(y_test, preds))




clf_0= KNeighborsClassifier(n_neighbors=3)
 
clf_0.fit(X_train, y_train)
 
score_0 = clf_0.score(X_train,y_train)
 
print("\n Metrica del modelo", score_0)
 
scores_0 = cross_val_score(clf_0, X_train, y_train, cv=kf, scoring="accuracy")
 
print("Metricas cross_validation", scores_0)
 
print("Media de cross_validation", scores_0.mean())
 
preds_0 = clf_0.predict(X_test)
 
score_pred_0 = metrics.accuracy_score(y_test, preds_0)
 
print("Metrica en Test", score_pred_0)

print(classification_report(y_test, preds_0))


clf_1 = SVC()
 
clf_1.fit(X_train, y_train)
 
score_1= clf_1.score(X_train,y_train)
 
print("\n Metrica del modelo", score)
 
scores_1 = cross_val_score(clf_1, X_train, y_train, cv=kf, scoring="accuracy")
 
print("Metricas cross_validation", scores_1)
 
print("Media de cross_validation", scores_1.mean())
 
preds_1 = clf_1.predict(X_test)
 
score_pred_1 = metrics.accuracy_score(y_test, preds_1)
 
print("Metrica en Test", score_pred_1)

print(classification_report(y_test, preds_1))

from sklearn import datasets, metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.svm import SVC

x=np.array(df_final.drop(['ahorro'],1)) 
y=np.array(df_final['ahorro'])
 
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
 
kf = KFold(n_splits=5)
 
clf_1 = SVC()
 
clf_1.fit(X_train, y_train)
 
score = clf_1.score(X_train,y_train)
 
print("Metrica del modelo", score)
 
scores_1 = cross_val_score(clf_1, X_train, y_train, cv=kf, scoring="accuracy")
 
print("Metricas cross_validation", scores_1)
 
print("Media de cross_validation", scores_1.mean())
 
preds_1 = clf_1.predict(X_test)
 
score_pred_1 = metrics.accuracy_score(y_test, preds_1)
 
print("Metrica en Test", score_pred_1)